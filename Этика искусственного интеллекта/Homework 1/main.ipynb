{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf61c67",
   "metadata": {},
   "source": [
    "# Домашнее задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b8fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fb9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same results each time\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_csv(\"./data/data.csv\")\n",
    "comments = df[\"comment_text\"]\n",
    "target = (df[\"target\"]>0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb07274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 1\n",
    "# Делим выборку на обучающую и тестовую\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff588fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2\n",
    "# Векторизируем комментарии\n",
    "vect = CountVectorizer()\n",
    "train = vect.fit_transform(X_train['comment_text'])\n",
    "test = vect.transform(X_test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5f0b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9287888232921419"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание 3\n",
    "# Строим модель логистической регрессии \n",
    "log = LogisticRegression(max_iter=2000)\n",
    "log.fit(train, y_train)\n",
    "predict = log.predict(test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfab438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 4\n",
    "# Функция анализа пользовательского комментария\n",
    "def analyze_comment_proba(comment, vect, model):\n",
    "    vector = vect.transform([comment])\n",
    "    predict = model.predict_proba(vector)\n",
    "    return predict[0][1]\n",
    "\n",
    "def predict_comment_toxic(comment, vect, model):\n",
    "    vector = vect.transform([comment])\n",
    "    predict = model.predict(vector)\n",
    "    return predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e33a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность того, что комментарий Apples are stupid токсичный:  0.99945\n",
      "Вероятность того, что комментарий I love apples токсичный:  0.09842\n",
      "Комментарий Apples are stupid  токсичный\n",
      "Комментарий I love apples  не токсичный\n"
     ]
    }
   ],
   "source": [
    "# Задание 5\n",
    "# Определение пользовательских комментариев\n",
    "comment_1 = 'Apples are stupid'\n",
    "comment_2 = 'I love apples'\n",
    "print(f'Вероятность того, что комментарий {comment_1} токсичный: ',\n",
    "      analyze_comment_proba(comment_1, vect, log).round(5))\n",
    "print(f'Вероятность того, что комментарий {comment_2} токсичный: ',\n",
    "      analyze_comment_proba(comment_2, vect, log).round(5))\n",
    "print(f'Комментарий {comment_1} ', 'токсичный' if predict_comment_toxic(comment_1, vect, log) else 'не токсичный')\n",
    "print(f'Комментарий {comment_2} ', 'токсичный' if predict_comment_toxic(comment_2, vect, log) else 'не токсичный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5db5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 слов, указывающих на класс 1:\n",
      "stupid\n",
      "idiot\n",
      "idiots\n",
      "stupidity\n",
      "idiotic\n",
      "crap\n",
      "dumb\n",
      "pathetic\n",
      "morons\n",
      "moron\n"
     ]
    }
   ],
   "source": [
    "# Задание 6\n",
    "# Определение самых токсичных слов для модели\n",
    "log_coef = log.coef_[0]\n",
    "word_coef = {}\n",
    "for word, idx in vect.vocabulary_.items():\n",
    "    word_coef[word] = log_coef[idx]\n",
    "\n",
    "sorted_word_coef = sorted(word_coef.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "positive_coefs = sorted(word_coef.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nТоп-10 слов, указывающих на класс 1:\")\n",
    "for word, coef in positive_coefs[:10]:\n",
    "    print(f\"{word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3292eb",
   "metadata": {},
   "source": [
    "Задание 7\n",
    "\n",
    "Все слова, которые перечислены в списке выше подходят под определение токсичных, никаких неожиданных слов обнаружено не было."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "547d043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность того, что комментарий I have a christian friend токсичный:  0.12294\n",
      "Вероятность того, что комментарий I have a muslim friend токсичный:  0.4482\n",
      "Вероятность того, что комментарий I have a white friend токсичный:  0.32911\n",
      "Вероятность того, что комментарий I have a black friend токсичный:  0.5163\n",
      "Комментарий I have a christian friend  не токсичный\n",
      "Комментарий I have a muslim friend  не токсичный\n",
      "Комментарий I have a white friend  не токсичный\n",
      "Комментарий I have a black friend  токсичный\n"
     ]
    }
   ],
   "source": [
    "# Задание 8\n",
    "# Проверка на этичность\n",
    "\n",
    "ethic_comment_1 = \"I have a christian friend\"\n",
    "ethic_comment_2 = \"I have a muslim friend\"\n",
    "ethic_comment_3 = \"I have a white friend\"\n",
    "ethic_comment_4 = \"I have a black friend\"\n",
    "\n",
    "print(f'Вероятность того, что комментарий {ethic_comment_1} токсичный: ',\n",
    "      analyze_comment_proba(ethic_comment_1, vect, log).round(5))\n",
    "print(f'Вероятность того, что комментарий {ethic_comment_2} токсичный: ',\n",
    "      analyze_comment_proba(ethic_comment_2, vect, log).round(5))\n",
    "print(f'Вероятность того, что комментарий {ethic_comment_3} токсичный: ',\n",
    "      analyze_comment_proba(ethic_comment_3, vect, log).round(5))\n",
    "print(f'Вероятность того, что комментарий {ethic_comment_4} токсичный: ',\n",
    "      analyze_comment_proba(ethic_comment_4, vect, log).round(5))\n",
    "\n",
    "print(f'Комментарий {ethic_comment_1} ', 'токсичный' if predict_comment_toxic(ethic_comment_1, vect, log) else 'не токсичный')\n",
    "print(f'Комментарий {ethic_comment_2} ', 'токсичный' if predict_comment_toxic(ethic_comment_2, vect, log) else 'не токсичный')\n",
    "print(f'Комментарий {ethic_comment_3} ', 'токсичный' if predict_comment_toxic(ethic_comment_3, vect, log) else 'не токсичный')\n",
    "print(f'Комментарий {ethic_comment_4} ', 'токсичный' if predict_comment_toxic(ethic_comment_4, vect, log) else 'не токсичный')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a7ce2",
   "metadata": {},
   "source": [
    "В результате видим, что комментарий I have a christian friend наименее токсичный из всех, хотя смысловая нагрузка во всех комментариях одинкова (а комментарий I have a black friend вообще пометили, как токсичный). Следовательно, у модели есть bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bb2fa",
   "metadata": {},
   "source": [
    "Задание 9\n",
    "\n",
    "В тексте дано условие о том, что сообщество, представленное в выборке, исламофобно. Следовательно, это предвзятость социально-историческую предвзятость, которая связана с самими людьми, которые писали данные комментарии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3314ce7",
   "metadata": {},
   "source": [
    "Задание 10\n",
    "\n",
    "Идея 1: кажется, что самым простым и эффективным решением будет добавить в выборку некоторые положительные комментарии об исламе (если в остальном модель нас полностью устраивает). Либо можно исключить часть токсичных комментариев, связанных с исламом.\n",
    "\n",
    "Идея 2: изменить модель с обычной логистической регрессии на более сложную, например, трансформерные модели. Тогда модель сможет улавливать контекст в словах, а не просто анализировать предложение по ключевым словам. Например, модель BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c19d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
